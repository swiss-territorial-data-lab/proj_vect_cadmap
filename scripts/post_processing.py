import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"

import cv2
import argparse
import numpy as np
import os.path as osp
import geopandas as gpd
import rasterio as rio

from osgeo import gdal
from statistics import mode
from affine import Affine
from tqdm import tqdm
from PIL import ImageDraw, Image

Image.MAX_IMAGE_PIXELS = None

from matplotlib import pyplot as plt
from shapely.ops import nearest_points
from shapely.geometry import box, Polygon
from shapely.affinity import affine_transform
from rasterio.mask import geometry_mask

import easyocr
reader = easyocr.Reader(['fr'], gpu=False) # this needs to run only once to load the model into memory


def parse_args():
    parser = argparse.ArgumentParser(description='Delineate the raster mask generated by the deep learning models.')
    parser.add_argument('--arcgis',
                        default="./data/arcgis/",
                        help='the path to load vectorized polygons in image frame from arcgis.')
    parser.add_argument('--tif',
                        default="./data/",
                        help='the path to load initial GeoTOFF file ')
    parser.add_argument('--raster_tif',
                        default="./data/raster_tif/",
                        help='the path to load GeoTOFF file in pixel coordinates / image frame.')    
    parser.add_argument('--semantic',
                        default="./data/semantic_prediction_mask/",
                        help='the path to load semantic prediction masks')
    parser.add_argument('--ocr',
                        default=True,
                        help='whether to recognize the parcel and building number of the vectorized polygons.')
    parser.add_argument('--method',
                        default='elementary',
                        required=True,
                        choices=['elementary', 'sophisticated'],
                        help='elementary / sophisticated layer from arcgis .gdb file to aggregate.')
    parser.add_argument('--save',
                        default="./output/",
                        help='the path to save segmented tiles')
    args = parser.parse_args()

    return args


def iou(bbox1, bbox2):
    # Define a function to calculate the IoU between two bounding boxes
    # Get the coordinates of the intersection rectangle
    x1 = max(bbox1[0][0], bbox2[0][0])
    y1 = max(bbox1[0][1], bbox2[0][1])
    x2 = min(bbox1[2][0], bbox2[2][0])
    y2 = min(bbox1[2][1], bbox2[2][1])
    # If there is no intersection, return 0
    if x1 >= x2 or y1 >= y2:
        return 0
    # Calculate the area of the intersection rectangle
    intersection_area = (x2 - x1) * (y2 - y1)
    # Calculate the area of both bounding boxes
    bbox1_area = (bbox1[2][0] - bbox1[0][0]) * (bbox1[2][1] - bbox1[0][1])
    bbox2_area = (bbox2[2][0] - bbox2[0][0]) * (bbox2[2][1] - bbox2[0][1])
    # Calculate the IoU as the ratio of intersection area over union area
    union_area = bbox1_area + bbox2_area - intersection_area
    iou = intersection_area / union_area
    return iou


class TextExtractor():
    ocr_reader = easyocr.Reader(['fr', 'en'])

    def draw_bounding_box(self, img, weight, x_left, y_lower, x_right, y_upper):
        cv2.rectangle(img, (x_left, y_lower), (x_right, y_upper),
                      (200, 200, 0), weight)
        return img

    def draw_contour(self, img, weight, pts):
        # Polygon corner points coordinates
        pts = np.array(pts, np.int32)
        pts = pts.reshape((-1, 1, 2))

        # the last line connected to the first line.
        isClosed = True

        # Green color in BGR
        color = (200, 0, 0)

        # Using cv2.polylines() method
        # Draw a Blue polygon with
        # thickness of 1 px
        img = cv2.polylines(img, [pts], isClosed, color, weight)
        return img

    def draw_text(self, image, text, weight, x_left, y_lower):
        # font = ImageFont.truetype(size=weight)
        img_pil = Image.fromarray(image)
        draw = ImageDraw.Draw(img_pil)

        r, g, b, a = 250, 0, 0, 0
        draw.text((x_left, y_lower - weight), text, fill=(r, g, b, a))
        # draw.text((x_left, y_lower - weight), text, font=font, fill=(r, g, b, a))
        texted_image = np.array(img_pil)

        return texted_image

    def display_image(self, image, title):
        plt.imshow(image)
        plt.title(title)
        plt.show(block=False)

        keyboardClick = False
        while keyboardClick != True:
            keyboardClick = plt.waitforbuttonpress()
        plt.close()


    def recognize_text(self, img) -> list:
        try:
            results = self.ocr_reader.readtext(img, detail=1, paragraph=False, allowlist='123456789', 
                                               blocklist='abcdefghijklmnopqrstuvwxyz~!@#$%^&*()_+{}|:"<>?`-=[]\;.',
                                               text_threshold=0.2, ycenter_ths=0.9)

        except Exception as err:
            print(f" Exception while readtext(): {str(err)}")

        return results


    def display_results(self, img, rsts: list):
        assemble_img = img
        for rst in rsts:
            contour, text = rst[0], rst[1]
            assemble_img = self.draw_contour(assemble_img, 1, contour)

            assemble_img = self.draw_text(assemble_img, text, 0, contour[0][0], contour[0][1])

        return assemble_img
        

# Define a function to calculate the intersection over union (IoU) of two polygons
def intersect_r(bbox, polygon):
    intersection = bbox * polygon
    return np.sum(intersection) / np.sum(bbox)


def transform_polygon(polygon, fwd):
    # Get the exterior and interior coordinates of the polygon
    exterior = polygon.exterior.coords
    interiors = [interior.coords for interior in polygon.interiors]
    # Inverse the y coordinate of each point
    # (coords[0], -coords[1]) refers to (x, -y, _) as there might has or not z
    new_exterior = [fwd * (coords[0], -coords[1]) for coords in exterior]
    new_interiors = [[fwd * (coords[0], -coords[1]) for coords in interior] for interior in interiors]
    # Create a new polygon with the inversed coordinates
    new_polygon = Polygon(new_exterior, new_interiors)

    # Return a new multipolygon from the list of transformed polygons
    return new_polygon


def main():

    # init argparse 
    args = parse_args()
    ocr = args.ocr
    method = args.method
    arcgis_path = args.arcgis
    tif_folder = args.tif
    raster_tif = args.raster_tif
    semantic_path = args.semantic
    save_path = args.save

    # Define a dictionary of semantic classes based on pixel values
    semantic_classes = {
        0: [0, 0, 0], # Background
        1: [255, 255, 255], # Borderline
        2: [255, 115, 223], # Building
        3: [211, 255, 190], # Unbuilt
        4: [78, 78, 78], # Wall
        5: [255, 255, 0], # Road
        6: [190, 232, 255] # River
    }
    
    origin_plan_ls = os.listdir(tif_folder)

    origin_plan_ls = [file for file in origin_plan_ls if file.endswith('.tif') and not file.endswith('Assemblage.tif')]

    for origin_plan in tqdm(origin_plan_ls):
        plan_name = origin_plan.split('.')[0]

        polygons = gpd.read_file(osp.join(arcgis_path, plan_name, plan_name + '.gdb'), layer='polygon_{}'.format(method)).explode(index_parts=True)
        polygons = polygons.assign(ParcelNum=None)
        polygons = polygons.assign(ParcelCof=None)
        polygons = polygons.assign(BuildNum=None)
        polygons = polygons.assign(BuildCof=None)
        polygons = polygons.assign(CLASS=None)
        
        # Read the raster map layer
        raster_map = rio.open(osp.join(raster_tif, origin_plan))
        
        # Read the semantic raster layer
        semantic_map = cv2.imread(osp.join(semantic_path, plan_name + '.png'))
        
        h, w, _ = semantic_map.shape
        semantic_mask = np.zeros((h,w))
        
        for key, color in semantic_classes.items():
            semantic_mask[(semantic_map == [color[2],color[1],color[0]]).all(axis=2)] = key
            
        # OCR
        if ocr:
            text_extractor = TextExtractor()

            # Define the parameters
            input_size = 2560 # The size of the input image for the ocr_reader function
            slide_size = 2000 # The size of the sliding window
            iou_threshold = 0.3 # The IoU threshold to consider two bounding boxes as duplicates

            # Open the image and get its size
            image = cv2.imread(osp.join(tif_folder, origin_plan))
            height, width, _ = image.shape

            # Initialize a list to store the results
            results = []

            # Loop over the image with a sliding window
            for x in range(0, width, slide_size):
                for y in range(0, height, slide_size):
                    # Crop the image patch
                    patch = image[y:y + input_size, x:x + input_size]
                    # Run the ocr_reader function on the patch
                    patch_results = text_extractor.recognize_text(patch)
                    # Loop over the patch results
                    for bbox, text, confidence in patch_results:
                        # Validate detected number within correct range
                        if text == '':
                            continue
                        # Calculate the location of the bounding box in the original image
                        bbox = [(x + bx, y + by) for bx, by in bbox]
                        # Check if the result is already in the results list
                        duplicate = False
                        for i, (old_bbox, old_text, old_confidence) in enumerate(results):
                            # If the text is the same and the IoU is above the threshold, keep only the result with higher confidence
                            if iou(bbox, old_bbox) > iou_threshold:
                                duplicate = True
                                if confidence > old_confidence:
                                    results[i] = (bbox, text, confidence)
                                break
                        # If not a duplicate, append the result to the results list
                        if not duplicate:
                            results.append((bbox, text, confidence))
            
            bboxes = []

            # Read the raster map layer
            red_channel = image[:,:,2]

            # Iterate over all the bounding boxes
            for bbox, text, cof in tqdm(results):
                # Create a shapely polygon from the bounding box coordinates
                bbox_shp = Polygon(bbox)
                bbox_raster = rio.features.rasterize([(bbox_shp, 1)], out_shape=semantic_mask.shape)
                bbox_raster = np.bool_(bbox_raster)
                # Extract the pixel values within the bounding box from the raster map layer
                pixel_values = red_channel[bbox_raster]
                color_stats = np.quantile(pixel_values, 0.05)
                if color_stats >= 200:
                    bboxes.append({"coords": bbox, "value": int(text), "confidence": cof, "color": 'red'})
                elif color_stats < 200:
                    bboxes.append({"coords": bbox, "value": int(text), "confidence": cof, "color": 'black'})        
        
        # Iterate over all the polygons in the feature class
        for polygon in tqdm(polygons.itertuples()):
        
            # Convert the polygon geometry to raster coordinates
            polygon_raster = geometry_mask([polygon.geometry], out_shape=raster_map.shape, transform=raster_map.transform, invert=True)
            if np.sum(polygon_raster) == 0:
                continue
            
            # Extract the pixel values from the semantic map layer within the polygon
            pixel_values = semantic_mask[polygon_raster]
        
            # Calculate the majority pixel value
            majority_value = mode(pixel_values)
        
            # Assign the semantic class to the polygon based on the majority pixel value
            polygons.loc[polygon.Index, "CLASS"] = majority_value

            if ocr == True:
                # Iterate over all the bounding boxes
                for bbox in bboxes:
                    # Create a shapely polygon from the bounding box coordinates
                    bbox_shp = Polygon(bbox["coords"])

                    # Convert the bounding box to raster coordinates
                    bbox_raster = rio.features.rasterize([(bbox_shp, 1)], out_shape=semantic_mask.shape)
                    bbox_raster = np.bool_(bbox_raster)

                    # Calculate the Ratio of intersection between the polygon and the bounding box
                    roi = intersect_r(bbox_raster, polygon_raster)

                    # If the IoU is greater than 0.7, assign the parcel number and confidence to the polygon
                    if roi > 0.7:
                        if bbox["color"] == 'black':
                            # If there is already a parcel number assigned, compare the confidences and keep the higher one
                            if polygon.ParcelNum is not None:
                                if bbox["confidence"] > polygon.ParcelCof:
                                    polygons.loc[polygon.Index, "ParcelNum"] = bbox["value"]
                                    polygons.loc[polygon.Index, "ParcelCof"] = bbox["confidence"] 
                            # If there is no parcel number assigned, assign the current one
                            else:
                                polygons.loc[polygon.Index, "ParcelNum"] = bbox["value"]
                                polygons.loc[polygon.Index, "ParcelCof"] = bbox["confidence"] 
                                
                        elif bbox["color"] == 'red' and polygon.CLASS == 2:
                            # If there is already a parcel number assigned, compare the confidences and keep the higher one
                            if polygon.BuildingNum is not None:
                                if bbox["confidence"] > polygon.BuildingCof:
                                    polygons.loc[polygon.Index, "BuildNum"] = bbox["value"]
                                    polygons.loc[polygon.Index, "BuildCof"] = bbox["confidence"] 
                            # If there is no parcel number assigned, assign the current one
                            else:
                                polygons.loc[polygon.Index, "BuildNum"] = bbox["value"]
                                polygons.loc[polygon.Index, "BuildCof"] = bbox["confidence"] 

        # project the vectorized results to spacial coordinate system 
        geodata = gdal.Open(osp.join(tif_folder, origin_plan))
        # geometries = geoTransform(shapes, geodata)
        GT = geodata.GetGeoTransform()
        fwd = Affine.from_gdal(*GT)
        # Apply the function to the geometry column of the GeoDataFrame
        polygons["geometry"] = polygons["geometry"].apply(transform_polygon, args=[fwd])

        # shp file can not store datetime 
        if 'TIME' in polygons.columns:
            polygons.drop(columns='TIME', inplace=True)
        # Save the aggregated results to the a shp file
        if not os.path.exists(osp.join(save_path, plan_name)):
            os.mkdir(osp.join(save_path, plan_name))
        polygons.to_file(osp.join(save_path, plan_name, 'projected_{}.shp'.format(method)))


if __name__ == '__main__':
    main()